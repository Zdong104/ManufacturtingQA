QUALITY CONTROL AND INSPECTION Chapter Contents 42.1 Product Quality 42.2 Process Capability and Tolerances 42.3 Statistical Process Control 42.3.1 Control Charts for Variables 42.3.2 Control Charts for Attributes 42.3.3 Interpreting the Charts 42.4 Quality Programs in Manufacturing 42.4.1 Total Quality Management 42.4.2 Six Sigma 42.4.3 Taguchi Methods 42.4.4 ISO 9000 42.5 Inspection Principles 42.5.1 Manual and Automated Inspection 42.5.2 Contact versus Noncontact Inspection 42.6 Modern Inspection Technologies 42.6.1 Coordinate Measuring Machines 42.6.2 Measurements with Lasers 42.6.3 Machine Vision 42.6.4 Other Noncontact Inspection Techniques Traditionally, quality control (QC) has been concerned with detecting poor quality in manufactured products and taking corrective action to eliminate it. QC has often been limited to inspecting the product and its components, and deciding whether the dimensions and other features conformed to design specifications. If they did, the product was shipped. The modern view of quality control encompasses a broader scope of activities, including various quality programs such as statistical process control and Six Sigma as well as modern inspection technologies such as coordinate measuring machines and machine vision. In this chapter, we discuss these and other quality and inspection topics that are relevant today in modern manufacturing operations. Let us begin our coverage by defining product quality. 42.1 PRODUCT QUALITY The dictionary defines quality as ‘‘the degree of excellence which a thing possesses,’’ or ‘‘the features that make something what it is’’—its characteristic elements and attributes. The American Society for Quality (ASQ) defines quality as ‘‘the totality of features and characteristics of a product or service that bear on its ability to satisfy given needs’’ [2]. In a manufactured product, quality has two aspects [4]: (1) product features and (2) freedom from deficiencies. Product features are the characteristics of the product that result from design. They are the functional and aesthetic features of the item intended to appeal to and provide satisfaction tothe customer. In anautomobile, these features includethesizeofthecar,itsstyling,thefinishofthebody,gas mileage, reliability, reputation of the manufacturer, and similar aspects. They also include the available options for the customer to choose. The sum of a product’s features usually defines its grade, which relates to the level in the market at which the product is aimed. Cars (and most other products) come in different grades. Some cars provide basic transportation because that is what some customers want, while others are upscale for consumers willing to spend more to own a ‘‘better product.’’ The features of a product are decided in design, and they generally determine the inherent cost of the product. Superior features and more of them mean higher cost. Freedomfromdeficiencies means that the product does what itissupposed to do (within the limitations of its design features), that it is absent of defects and out-of-tolerance conditions, and that no parts are missing. This aspect of quality includes components and subassemblies of the product as well as the product itself. Freedom from deficiencies means conforming to design specifications, which is accomplished in manufacturing. Although the inherent cost to make a product is a function of its design, minimizing the product’s cost to the lowest possible level within the limits set by its design is largely a matter of avoiding defects, tolerance deviations, and other errors during production. Costs of these deficiencies make a long list indeed: scrapped parts, larger lot sizes for scrap allowances, rework, reinspection, sortation, customer complaintsand returns,warrantycosts and customer allowances, lost sales, and lost good will in the marketplace. Thus, product features are the aspect of quality for which the design department is responsible. Product features determine to a large degree the price that a company can charge for its products. Freedom from deficiencies is the quality aspect for which the manufacturing departments are responsible. The ability to minimize these deficiencies has an important influence on the cost of the product. These generalities oversimplify the way things work, because the responsibility for high quality extends well beyond the design and manufacturing functions in an organization. In any manufacturing operation, variability exists in the process output. In a machining operation, which is one of the most accurate processes, the machined parts may appear to be identical, but close inspection reveals dimensional differences from one part to the next. Manufacturing variations can be divided into two types: random and assignable. Random variations are caused by many factors: human variability within each operation cycle, variations in raw materials, machine vibration, and so on. Individually, these factors may not amount to much, but collectively the errors can be significant enough to cause trouble unless they are within the tolerances for the part. Random variations typically form a normal statistical distribution. The output of the process tends to cluster about the mean value, in terms of the product’s quality characteristic of interest (e.g., length, diameter). Alarge proportion of the population of parts is centered around the mean, with fewer parts away from the mean. When the only variations in the process are of this type, the process is said to be in statistical control.This kind of variability will continue so long as the process is operating normally. It is when the process deviates from this normal operating condition that variations of the second type appear. Assignable variations indicate an exception from normal operating conditions. Something has occurred in the process that is not accounted for by random variations. Reasons for assignable variations include operator mistakes, defective raw materials, tool failures, machine malfunctions, and so on. Assignable variations in manufacturing usually betray themselves by causing the output to deviate from the normal distribution. The process is no longer in statistical control. Process capability relates to the normal variations inherent in the output when the where PC ¼ process capability; m ¼ process mean, which is set at the nominal value of the product characteristic when bilateral tolerancing is used (Section 5.1.1); and s ¼ standard deviation of the process. Assumptions underlying this definition are (1) steady state operation has been achieved and the process is in statistical control, and (2) the output is normally distributed. Under these assumptions, 99.73% of the parts produced will have output values that fall within 3.0s of the mean. process is in statistical control. By definition, process capability equals  3 standard  deviations about the mean output value (a total of 6 standard deviations),  PC ¼ m  3 s  ð42:1Þ  The process capability of a given manufacturing operation is not always known, and experiments must be conducted to assess it. Methods are available to estimate the natural tolerance limits based on a sampling of the process. The issue of tolerances is critical to product quality. Design engineers tend to assign dimensional tolerances to components and assemblies based on their judgment of how size variations will affect function and performance. Conventional wisdom is that closer tolerances beget better performance. Small regard is given to the cost resulting from tolerances that are unduly tight relative to process capability. As tolerance is reduced, the cost of achieving the tolerance tends to increase because additional processing steps may be needed and/or more accurate and expensive production machines may be required. The design engineer should be aware of this relationship. Although primary consideration must be given to function in assigning tolerances, cost is also afactor, and any reliefthat can be given to the manufacturing departments in the form of widertoleranceswithout sacrificing product function is worthwhile. Design tolerances must be compatible with process capability. It serves no useful purpose to specify a tolerance of 0.025 mm ( 0.001 in) on a dimension if the process capability is significantly wider than 0.025 mm ( 0.001 in). Either the tolerance should be opened further (if design functionality permits), or a different manufacturing process should be selected. Ideally, the specified tolerance should be greater than the process capability. If function and available processes prevent this, then sorting must be included in the manufacturing sequence to inspect every unit and separate those that meet specification from those that do not. Design tolerances can be specified as being equal to process capability as defined in Eq. (42.1). The upper and lower boundaries of this range are known as the natural tolerance limits. When design tolerances are set equal to the natural tolerance limits, then 99.73% of the parts will be within tolerance and 0.27% will be outside the limits. Any increase in the tolerance range will reduce the percentage of defective parts. Tolerances are not usually set at their natural limits by product design engineers; tolerances are specified based on the allowable variability that will achieve required function and performance. It is useful to know the ratio of the specified tolerance relative to the process capability. This is indicated by the process capability index T PCI ¼ð42:2Þ 6s where PCI ¼ process capability index; T ¼ tolerance range—the difference between upper and lower limits of the specified tolerance; and 6s ¼ natural tolerance limits. The underlying assumption in this definition is that the process mean is set equal to the nominal design specification, so that the numerator and denominator in Eq. (42.2) are centered about the same value. Table 42.1 shows the effect of various multiples of standard deviation on defect rate (i.e., proportion of out-of-tolerance parts). The desire to achieve very-low-fraction defect rates has led to the popular notion of ‘‘six sigma’’ limits in quality control. Achieving Six Sigma limits virtually eliminates defects in a manufactured product, assuming the process is maintained within statistical control. As we shall see later in the chapter, Six Sigma quality programs do not quite live up to their names. Before addressing that issue, let us discuss a widely used quality control technique: statistical process control. TABLE 42.1 Defect rate when tolerance is deﬁned in terms of number of standard deviations of the process, given that the process is operating in statistical control. No. of Standard Process Capability Defect Defective Parts Deviations Index Rate, % per Million 1.0 0.333 31.74% 317,400 2.0 0.667 4.56% 45,600 3.0 1.00 0.27% 2,700 4.0 1.333 0.0063% 63 5.0 1.667 0.000057% 0.57 6.0 2.00 0.0000002% 0.002 Statistical process control (SPC) involves the use of various statistical methods to assess and analyze variations in a process. SPC methods include simply keeping records of the production data, histograms, process capability analysis, and control charts. Control charts are the most widely used SPC method, and this section will focus on them. The underlying principle in control charts is that the variations in any process divide into two types (Section 42.2): (1) random variations, which are the only variations present if the process is in statistical control, and (2) assignable variations that indicate a departure from statistical control. It is the objective of a control chart to identify when the process has gone out of statistical control, thus signaling that some corrective action should be taken. A control chart is a graphical technique in which statistics computed from measured values of a certain process characteristic are plotted over time to determine if the process remains in statistical control. The general form of the control chart is illustrated in Figure 42.1. The chart consists of three horizontal lines that remain constant over time: a center, a lower control limit (LCL), and an upper control limit (UCL). The center is usually set at the nominal design value. The upper and lower control limits are generally set at 3standard deviations of the sample means. It is highly unlikely that a random sample drawn from the process will lie outside the upper or lower control limits while the process is in statistical control. Thus, if it happens that a sample value does fall outside these limits, it is interpreted to mean that the process is out of control. Therefore, an investigation is undertaken to determine the reason for the out-of-control condition, with appropriate corrective action to eliminate the condition. By similar reasoning, if the process is found to be in statistical control, and there is no evidence of undesirable trends in the data, then no adjustments should be made since they would introduce an assignable variation to the process. The philosophy, ‘‘If it ain’t broke, don’t fix it,’’ is applicable to control charts. There are two basic types of control charts: (1) control charts for variables and (2) control charts for attributes. Control charts for variables require a measurement of the quality characteristic of interest. Control charts for attributes simply require a determination of whether a part is defective or how many defects there are in the sample. 42.3.1 CONTROL CHARTS FOR VARIABLES A process that is out of statistical control manifests this condition in the form of significant changes in process mean and/or process variability. Corresponding to these possibilities, there are two principal types of control charts for variables: x chart and R chart. The x chart (call it ‘‘x bar chart’’) is used to plot the average measured value of a certain quality characteristic for each of a series of samples taken from the production process. It indicates how the process mean changes over time. The R chart plots the range of each sample, thus monitoring the variability of the process and indicating whether it changes over time. A suitable quality characteristic of the process must be selected as the variable to be monitored on the x and R charts. In a mechanical process, this might be a shaft diameter or other critical dimension. Measurements of the process itself must be used to construct the two control charts. With the process operating smoothly and absent of assignable variations, a series of samples (e.g., m ¼ 20 or more is generally recommended) of small size (e.g., n ¼ 4, 5, or 6 parts per sample) are collected and the characteristic of interest is measured for each part. The following procedure is used to construct the center, LCL, and UCL for each chart: 1. Compute the mean x and range R for each of the m samples. 2. Compute the grand mean x, which is the mean of the x values for the m samples; this will be the center for the x chart. 3. Compute R, which is the mean of the R values for the m samples; this will be the center for the R chart. 4. Determine the upper and lower control limits, UCL and LCL, for the x and R charts. The approach is based on statistical factors tabulated in Table 42.2 that have been derived specifically for these control charts. Values of the factors depend on sample size n.For the x TABLE 42.2  Constants for the x and R charts.  R Chart  Sample  x Chart  Size n  A2  D3  D4  3  1.023  0  2.574  4  0.729  0  2.282  5  0.577  0  2.114  6  0.483  0  2.004  7  0.419  0.076  1.924  8  0.373  0.136  1.864  9  0.337  0.184  1.816  10  0.308  0.223  1.777  Example 42.1 x and R Charts chart: LCL ¼ x  A2R and  UCL ¼ x A2R  ð42:3Þ  and for the R chart:  LCL ¼ D3R  and  UCL ¼ D4R  ð42:4Þ  Eight samples (m ¼ 8) of size 4 (n ¼ 4) have been collected from a manufacturing process that is in statistical control, and the dimension of interest has been measured for each part. It is desired to determine the values of the center, LCL, and UCL for x and R charts. The calculated x values (units are cm) for the eight samples are 2.008, 1.998, 1.993, 2.002, 2.001, 1.995, 2.004, and 1.999. The calculated R values (cm) are, respectively, 0.027, 0.011, 0.017, 0.009, 0.014, 0.020, 0.024, and 0.018. Solution: The calculation of x and R values above comprise step 1 in our procedure. In step 2, we compute the grand mean of the sample averages. x ¼ð2:008 þ 1:998 þþ 1:999Þ=8 ¼ 2:000 In step 3, the mean value of R is computed. R ¼ð0:027 þ 0:011 þþ 0:018Þ=8 ¼ 0:0175 In step 4, the values of LCL and UCL are determined based on factors in Table 42.2. First, using Eq. (42.3) for the x chart, LCL ¼ 2:000 0:729ð0:0175Þ¼ 1:9872 UCL ¼ 2:000 þ 0:729ð0:0175Þ¼ 2:0128 and for the R chart using Eq. (42.4), LCL ¼ 00ð :0175Þ¼ 0 UCL ¼ 2:282ð0:0175Þ¼ 0:0399 n The two control charts are constructed in Figure 42.2 with the sample data plotted in the charts. 42.3.2 CONTROL CHARTS FOR ATTRIBUTES Control charts for attributes do not use a measured quality variable; instead, they monitor the number of defects present in the sample or the fraction defect rate as the plotted statistic. Examples of these kinds of attributes include number of defects per automobile, fraction of bad parts in a sample, existence or absence of flash in plastic moldings, and number of flaws in a roll of sheet steel. The two principal types of control charts for attributes are the p chart, which plots the fraction defect rate in successive samples; and the c chart, which plots the number of defects, flaws, or other nonconformities per sample. p Chart In the p chart, the quality characteristic of interest is the proportion (p for proportion) of nonconforming or defective units. For each sample, this proportion pi is the ratio of the number of nonconforming or defective items di over the number of units in the sample n (we assume samples of equal size in constructing and using the control chart) di pi ¼ð42:5Þ n where i is used to identify the sample. If the pi values for a sufficient number of samples are averaged, the mean value p is a reasonable estimate of the true value of p for the process. The p chart is based on the binomial distribution, where p is the probability of a nonconforming unit. The center in the p chart is the computed value of p for m samples of equal size n collected while the process is operating in statistical control. m pi p ¼ i¼1 ð42:6Þ Pm The control limits are computed as three standard deviations on either side of the center. Thus, ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ r  ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ r  LCL ¼ p  3  p 1ð n  pÞ  and  UCL ¼ p þ 3  p 1ð n  pÞ  ð42:7Þ  where the standard deviation of p in the binomial distribution is given by rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pð1 pÞ sp ¼ n If the value of p is relatively low and the sample size n is small, then the lower control limit computed by the first of these equations is likely to be a negative value. In this case, let LCL ¼ 0 (the fraction defect rate cannot be less than zero). c Chart In the c chart (c for count), the number of defects in the sample are plotted over time. The sample may be a single product such as an automobile, and c ¼ number of quality defects found during final inspection. Or the sample may be a length of carpeting at the factory prior to cutting, and c ¼ number of imperfections discovered in that strip. The c chart is based on the Poisson distribution, where c ¼ parameter representing the number of events occurring within a defined sample space (defects per car, imperfections per specified length of carpet). Our best estimate of the true value of c is the mean value over a large number of samples drawn while the process is in statistical control: m ci Pi¼1 c ¼ð42:8Þ m This value of c is used as the center for the control chart. In the Poisson distribution, the standard deviation is the square root of parameter c. Thus, the control limits are: pﬃﬃﬃ pﬃﬃﬃ LCL ¼ c 3 c and UCL ¼ c þ 3 c ð42:9Þ 42.3.3 INTERPRETING THE CHARTS When control charts are used to monitor production quality, random samples are drawn from the process of the same size n used to construct the charts. For x and R charts, the x and R values of the measured characteristic are plotted on the control chart. By convention, the points are usually connected, as in our figures. To interpret the data, one looks for signs that indicate the process is not in statistical control. The most obvious sign is when x or R (or both) lie outside the LCL or UCL limits. This indicatesan assignable cause such asbad starting materials, new operator, broken tooling, or similar factors. An out-of-limit x indicates a shift in the process mean. An out-of-limit R shows that the variability of the process has changed. The usual effect is that R increases, indicating variabilityhas risen. Less obvious conditions may reveal process problems, even though the sample points lie within the 3s limits. These conditions include (1) trends or cyclical patterns in the data, which may mean wear or other factors that occur as a function of time; (2) sudden changes in average level of the data; and (3) points consistently near the upper or lower limits. The same kinds of interpretations that apply to the x chart and R chart are also applicable to the p chart and c chart. Statistical process control is widely used for monitoring the quality of manufactured parts and products. Several additional quality programs are also used in industry, and in this section we briefly describe four of them: (1) total quality management, (2) Six Sigma, (3) Taguchi methods, and (4) ISO 9000. These programs are not alternatives to statistical process control; in fact, the tools used in SPC are included within the methodologies of total quality management and Six Sigma. 42.4.1 TOTAL QUALITY MANAGEMENT Total quality management (TQM) is a management approach to quality that pursues three main goals: (1) achieving customer satisfaction, (2) encouraging the involvement of the entire workforce, and (3) continuous improvement. The customer and customer satisfaction are a central focus of TQM, and products are designed and manufactured with this focus in mind. The product must be designed with the features that customers want, and it must be manufactured free of deficiencies. Within the scope of customer satisfaction is the recognition that there are two categories of customers: (1) external customers and (2) internal customers. External customers are those who purchase the company’s products and services. Internal customers are inside the company, such as the company’s final assembly department which is the customer of the parts production departments. For the total organization to be effective and efficient, satisfaction must be achieved in both categories of customers. In TQM, worker involvement in the quality efforts of the organization extends from the top executives through all levels beneath. There isrecognition of the important influence that product design has on product quality and how decisions made during design affect the quality that can be achieved in manufacturing. In addition, production workers are made responsible for the quality of their own output, rather than rely on inspectors to uncover defects after the parts are already produced. TQM training, including use of the tools of statistical process control, is provided to all workers. The pursuit of high quality is embraced by every member of the organization. The third goal of TQM is continuous improvement; that is, adopting the attitude that it is always possible to make something better, whether it is a product or a process. Continuous improvement in an organization is generally implemented using worker teams that have been organized to solve specific problems that are identified in production. The problems are not limited to quality issues. They may include productivity, cost, safety, or any other area of interest to the organization. Team members are selected on the basis of their knowledge and expertise in the problem area. They are drawn from various departments and serve part-time on the team, meeting several times per month until they are able to make recommendations and/or solve the problem. Then the team is disbanded. 42.4.2 SIX SIGMA The Six Sigma quality program originated and was first used at Motorola Corporation in the 1980s. It has been adopted by many other companies in the United States and was briefly discussed in Section 1.5 as one of the trends in manufacturing. Six Sigma is quite similar to total quality management in its emphasis on management involvement, worker teams to solve specific problems, and the use of SPC tools such as control charts. The major difference between Six Sigma and TQM is that Six Sigma establishes measurable targets for quality based on the number of standard deviations (sigma s)awayfromthe mean in theNormal distribution. Six sigma implies near perfectionin the process in the normal distribution. A process operating at the 6s level in a Six Sigma program produces no more than 3.4 defects per million, where a defect is anything that might result in lack of customer satisfaction. As in TQM, worker teams participate in problem-solving projects. A project requires the Six Sigma team to (1) define the problem, (2) measure the process and assess current performance, (3) analyze the process, (4) recommend improvements, and (5) develop a control plan to implement and sustain the improvements. The responsibility of management in Six Sigma is to identify important problemsintheiroperationsand sponsorthe teamsto address those problems. Statistical Basis of Six Sigma An underlying assumption in Six Sigma is that the defects in any process can be measured and quantified. Once quantified, the causes of the defects can be identified, and improvements can be made to eliminate or reduce the defects. The effects of any improvements can be assessed using the same measurements in a before-andafter comparison. The comparison is often summarized as a sigma level; for example, the process is now operating at the 4.8-sigma level whereas before it was only operating at the 2.6-sigma level. The relationship between sigma level and defects per million (DPM) is listed in Table 42.3 for a Six Sigma program. We see that the DPM was previously at 135,666 defects per 1,000,000 in our example, whereas it has now been reduced to 108 DPM. 986 Chapter 42/Quality Control and Inspection  TABLE 42.3 Sigma Levels and Corresponding Defects Per Million in a Six Sigma Program. Sigma Defects per Sigma Defects per Sigma Defects per Sigma Defects per level million level million level million level million  6.0s 3.4 5.8s 8.5 4.8s 483 3.8s 10,724 2.8s 96,801 5.6s 21 4.6s 968 3.6s 17,864 2.6s 135,666 5.4s 48 4.4s 1,866 3.4s 28,716 2.4s 184,060 5.2s 108 4.2s 3,467 3.2s 44,565 2.2s 241,964 5.0s 233 4.0s 6,210 3.0s 66,807 2.0s 308,538  Atraditional measure for goodprocess quality is 3s (three sigma level). It implies that the process is stable and in statistical control, and the variable representing the output of the process is normally distributed. Under these conditions, 99.73% of the output will be within the 3s range, and 0.27% or 2700 parts per million will lie outside these limits (0.135% or 1350 parts per million beyond the upper limit and the same number beyond the lower limit). But wait a minute, if we look up 3.0 sigma in Table 42.3, we find that there are 66,807 defects per million. Why is there a difference between the standard normal distribution value (2700 DPM and the value given in Table 42.3 (66,807 DPM)? There are two reasons for this discrepancy. First, the values in Table 42.3 refer to only one tail of the distribution, so that an appropriate comparison with the standard normal tables would only use one tail of the distribution (1350 DPM). Second, and much more significant, is that when Motorola devised the Six Sigma program, they considered the operation of processes over long periods of time, and processes over long periods tend to experience shifts from their original process means. To compensate for these shifts, Motorola decided to adjust the standard normal values by 1.5s.To summarize, Table 42.3 includes only one tail of the normal distribution, and it shifts the distribution by 1.5 sigma relative to the standard normal distribution. These effects can be seen in Figure 42.3. Measuring the Sigma Level In a Six Sigma project, the performance level of the process of interest is reduced to a sigma level. This is done at two points during the project: (1) after measurements have been taken of the process as it is currently operating and (2) after process improvements have been made to assess the effect of the improvements. This provides a before-and-after comparison. High sigma values represent good performance; low sigma values mean poor performance. To find the sigma level, the number of defects per million must first be determined. There are three measures of defects per million used in Six Sigma. The first and most important is the defects per million opportunities (DPMO), which considers that there may be more than one type of defect that can occur in each unit (product or service). More complex products are likely to have more opportunities for defects, while simple products have fewer opportunities. Thus, DPMO accounts for the complexity of the product and allows entirely different kinds of products or services to be compared. Defects per million opportunities is calculated using the following equation: NdDPMO ¼ 1; 000; 000 ð42:10Þ NuNo M1 M2 1.50 FIGURE 42.3 Normal distribution shift by 1.5s from original mean and consideration of only one tail of the distribution (at right). Key: m1 ¼mean of original distribution, m2 ¼mean of shifted distribution, s ¼ standard distribution. Example 42.2 Determining the Sigma Level of a Process where Nd ¼total number of defects found, Nu ¼number of units in the population of interest, and No ¼number of opportunities for a defect per unit. The constant 1,000,000 converts the ratio into defects per million. Other measures besides DPMO are defects per million (DPM), which measures all of the defects found in the population, and defective units per million (DUPM), which counts the number of defective units in the population and recognizes that there may be more than one type of defect in any defective unit. The following two equations can be Nu used to compute DPM and DUPM:  DPM ¼ 1; 000; 000  Nd Nu  ð42:11Þ  DUPM ¼ 1; 000; 000  Ndu  ð42:12Þ  where Ndu ¼number of defective units in the population, and the other terms are the same as for Eq. (42.10). Once the values of DPMO, DPM, and DUPM have been determined, Table 42.3 can be used to convert these values to their corresponding sigma levels. A final assembly plant that makes dishwashers inspects for 23 features that are considered important for overall quality. During the previous month, 9056 dishwashers were produced. During inspection, 479 defects among the 23 features were found, and 226 dishwashers had one or more defect. Determine DPMO, DPM, and DUPM for these data and convert each to its corresponding sigma level. Solution: Summarizing the data, Nu ¼9056, No ¼23, Nd ¼479, and Ndu ¼226. Thus, 479 DPMO ¼1; 000; 000 ¼2300 9056 23ðÞ The corresponding sigma level is about 4.3 from Table 42.3. 479 DPM ¼1; 000; 000 ¼52; 893 9056 988 Chapter 42/Quality Control and Inspection The corresponding sigma level is about 3.1. 226 DUPM ¼1; 000; 000 ¼24; 956 9056 The corresponding sigma level is about 3.4. n 42.4.3 TAGUCHI METHODS Genichi Taguchi has had an important influence on the development of quality engineering, especially in the design area—both product design and process design. In this section we review two of the Taguchi methods: (1) the loss function and (2) robust design. More complete coverage can be found among our references [5], [10]. The Loss Function Taguchi defines quality as ‘‘the loss a product costs society from the time the product is released for shipment’’ [10]. Loss includes costs to operate, failure to function, maintenance and repair costs, customer dissatisfaction, injuries caused by poor design, and similar costs. Some of these losses are difficult to quantify in monetary terms, but they are nevertheless real. Defective products (or their components) that are exposed before shipment are not considered part of this loss. Instead, any expense to the company resulting from scrap or rework of defective product is a manufacturing cost rather than a quality loss. Loss occurs when a product’s functional characteristic differs from its nominal or target value. Although functional characteristics do not translate directly into dimensional features, the loss relationship is most readily understood in terms of dimensions. When the dimension of a component deviates from its nominal value, the component’s function is adversely affected. No matter how small the deviation, there is some loss in function. The loss increases at an accelerating rate as the deviation grows, according to Taguchi. If we let x ¼the quality characteristic of interest and N ¼its nominal value, then the lossfunction will be a U-shaped curve as in Figure 42.4(a). A quadratic equation can be used to describe this curve: 2LxðÞ¼kxð NÞð42:13Þ where L(x) ¼loss function; k ¼constant of proportionality; and x and N are defined above. At some level of deviation (x2 N) ¼ (x1 N), the loss will be prohibitive, and it is necessary to scrap or rework the product. This level identifies one possible way of specifying the tolerance limit for the dimension. In the traditional approach to quality control, tolerance limits are defined and any product within those limits is acceptable. Whether the quality characteristic (e.g., the dimension) is close to the nominal value or close to one of the tolerance limits, it is acceptable. Trying to visualize this approach in terms analogous to the preceding relation, we obtain the discontinuous loss function in Figure 42.4(b). The reality is that products closer to the nominal x1 N x2 x Nquadratic quality loss Tolerance Tolerance function. (b) Loss function limits limitsimplicit in traditional tolerance speciﬁcation. (a) (b) Example 42.3 Taguchi Loss Function specification are better quality and will provide greater customer satisfaction. In order to improve quality and customer satisfaction, one must attempt to reduce the loss by designing the product and process to be as close as possible to the target value. A certain product has a critical dimension that is specified as 20.00 0.04 cm. Repair records indicate that if the tolerance is exceeded, there is a 75% probability that the product will be returned to the manufacturer at a cost of $80 for replacement and shipping. (a) Estimate the constant kin the Taguchi loss function, Eq. (42.13). (b) Using the loss function constant determined in (a), what would be the value of the loss function if the company could maintain a tolerance of 0.01 cm instead of 0.04 cm? Solution: In Eq. (42.13), the value of (xN) is the tolerance 0.04 cm. The loss is the expected cost of replacement and shipping, which is calculated as follows: ELxðÞg ¼0:75 $80Þþ0:25ðÞ¼0 $60f ð Using this expected cost in the loss function, the value of kcan be determined as follows: 260 ¼kð0:04Þ¼0:0016k k¼60=0:0016 ¼$37; 500 Accordingly, the Taguchi loss function is L(x) ¼37,500(xN). (b) For a tolerance of 0.01 cm, the loss function is determined as follows: LxðÞ¼37; 500ð0:01Þ2 ¼37; 500ð0:0001Þ¼$3:75 This is a significant reduction from the $60.00 using a tolerance of 0.04 cm. n Robust Design A basic purpose of quality control is to minimize variations. Taguchi calls the variations noise factors. A noise factor is a source of variation that is impossible or difficult to control and that affects the functional characteristics of the product. Three types of noise factors can be distinguished: (1) unit-to-unit, (2) internal, and (3) external. Unit-to-unit noise factors consist of inherent random variations in the process or product caused by variability in raw materials, machinery, and human participation. These are noise factors we have previously called random variations in the process. They are associated with a production process that is in statistical control. Internal noise factors are sources of variation that are internal to the product or process. They include time-dependent factors such as wear of mechanical components, spoilage of raw materials, and fatigue of metal parts; and operational errors, such as improper settings on the product or machine tool. An external noise factor is a source of variation that is external to the product or process, such as outside temperature, humidity, raw material supply, and input voltage. Internal and external noise factors constitute what we have previously called assignable variations. A robust design is one in which the product’s function and performance are relatively insensitive to variations in design and manufacturing parameters. It involves the design of both the product and process so that the manufactured product will be relatively unaffected by all noise factors. An example of a robust product design is an automobile whose ignition starter works as well in Minneapolis, Minnesota in winter as in Meridian, Mississippi in summer. An example of robust process design is a metal extrusion operation that produces good product despite temperature variations in the starting billet. 42.4.4 ISO 9000 ISO 9000 is a set of international standards that relate to the quality of the products (and services, if applicable) delivered by a given facility. The standards were developed by the International Organization for Standardization (ISO), which is based in Geneva, Switzerland. ISO 9000 establishes standards for the systems and procedures usedby the facility that determine the quality of its products. ISO 9000 is not a standard for the products themselves. Its focus is on systems and procedures, which include the facility’s organizational structure, responsibilities, methods, and resources needed to manage quality. ISO 9000 is concerned with the activities used by the facility to ensure that its products achieve customer satisfaction. ISO 9000 can be implemented in two ways, formally and informally. Formal implementation means that the facilitybecomes registered, which certifies that the facility meets the requirements of the standard. Registration is obtained through a third-party agency that conducts on-site inspections and reviews the facility’s quality systems and procedures. A benefit of registration is that it qualifies the facility to do business with companies that require ISO 9000 registration, which is common in the European Economic Community where certain products are regulated and ISO 9000 registration is required for companies making these products. Informal implementation of ISO 9000 means that the facility practices the standards or portions thereof simply to improve its quality systems. Such improvements are worthwhile, even without formal certification, for companies desiring to deliver high quality products. Inspection involves the use of measurement and gaging techniques to determine whether a product, its components, subassemblies, or starting materials conform to design specifications. The design specifications are established by the product designer, and for mechanical products they refer to dimensions, tolerances, surface finish, and similar features. Dimensions, tolerances, and surface finish were defined in Chapter 5, and many of the measuring instruments and gages for assessing these specifications were described in that chapter. Inspection is performed before, during, and after manufacturing. The incoming materials and starting parts are inspected upon receipt from suppliers; work units are inspected at various stages during their production; and the final product should be inspected prior to shipment to the customer. We should clarify the distinction between inspection and testing, which is a closely related topic. Whereas inspection determines the quality of the product relative to design specifications, testing generally refers to thefunctional aspects of the product. Does the product operate the way it is supposed to operate? Will it continue to operate for a reasonable period of time? Will it operate in environments of extreme temperature and humidity? In quality control, testing is a procedure in which the product, subassembly, part, or material is observed under conditions that might be encountered during service. For example, a product might be tested by operating it for a certain period of time to determine whether it functions properly. If it passes the test, it is approved for shipment to the customer. Testing of a component or material is sometimes damaging or destructive. In these cases, the items must be tested on a sampling basis. The expense of destructive testing is significant, and great efforts are made to develop methods that do not destroy the item. These methods are referred to as nondestructive testing or nondestructive evaluation. Inspections divide into two types: (1) inspection by variables, in which the product or part dimensions of interest are measured by the appropriate measuring instruments; and (2) inspection by attributes, in which the parts are gauged to determine whether they are within tolerance limits. The advantage of measuring a part dimension is that data are obtained about its actual value. The data might be recorded over time and used to analyze trends in the manufacturing process. Adjustments in the process can be made based on the data so that future parts are produced closer to the nominal design value. When a part dimension is simply gaged, all that is known is whether it is within tolerance or too big or too small. On the positive side, gaging can be done quickly and at low cost. 42.5.1 MANUAL AND AUTOMATED INSPECTION Inspection procedures are often performed manually. The work is usually boring and monotonous, yet the need for precision and accuracy is high. Hours are sometimes required to measure the important dimensions of only one part. Because of the time and cost of manual inspection, statistical sampling procedures are generally used to reduce the need to inspect every part. Sampling versus 100% Inspection When sampling inspection is used, the number of parts in the sample is generally small comparedtothe quantity of parts produced. The sample size may be only 1% of the production run. Because not all of the items in the population are measured, there is a risk in any sampling procedure that defective parts will slip through. One of the goals in statistical sampling is to define the expected risk; that is, to determine the average defect rate that will pass through the sampling procedure. The risk can be reduced by increasing the sample size and the frequency with which samples are collected. But the fact remains that 100% good quality cannot be guaranteed in a sampling inspection procedure. Theoretically, the only way to achieve 100%good quality is by 100%inspection; thus, all defects are screened and only good parts passthroughthe inspectionprocedure. However, when 100% inspection is done manually, two problems are encountered. The first is the expense involved. Instead of dividing the cost of inspecting the sample over the number of parts in the production run, the unit inspection cost is applied to every part in the batch. Inspection cost sometimes exceeds the cost of making the part. Second, in 100% manual inspection, there are almost always errors associated with the procedure. The error rate depends on the complexity and difficulty of the inspection task and how much judgment is required by the human inspector. These factors are compounded by operator fatigue. Errors mean that a certain number of poor quality parts will be accepted and a certain number of good quality parts will be rejected. Therefore, 100% inspection using manual methods is no guarantee of 100% good quality product. Automated 100% Inspection Automation of the inspection process offers a possible way to overcome the problems associated with 100% manual inspection. Automated inspection is defined as automation of one or more steps in the inspection procedure, such as (1) automated presentation of parts by an automated handling system, with a human operator still performing the actual inspection process (e.g., visually examining parts for flaws); (2) manual loading of parts into an automatic inspection machine; and (3) fully automated inspection cell in which parts are both presented and inspected automatically. Inspection automation can also include (4) computerized data collection from electronic measuring instruments. Automated 100% inspection can be integrated with the manufacturing process to accomplish some action relative to the process. The actions can be one or both of the following: (1) parts sortation, and/or (2) feedback of data to the process. Parts sortation means separating parts into two or more quality levels. The basic sortation includes two levels: acceptable and unacceptable. Some situations require more than two levels, such as acceptable, reworkable, and scrap. Sortation and inspection may be combined in the same station. An alternative approach is to locate one or more inspections along the processing line, and instructions are sent to a sortation station at the end of the line indicating what action is required for each part. Feedback of inspection data to the upstream manufacturing operation allows compensating adjustments to be made in the process to reduce variability and improve quality. If inspection measurements indicate that the output is drifting toward one of the tolerance limits (e.g., due to tool wear), corrections can be made to process parameters to move the output toward the nominal value. The output is thereby maintained within a smaller variability range than possible with sampling inspection methods. 42.5.2 CONTACT VERSUS NONCONTACT INSPECTION There are a variety of measurement and gaging technologies available for inspection. The possibilities can be divided into contact and noncontact inspection methods. Contact inspection involves the use of a mechanical probe or other device that makes contact with the object being inspected. By its nature, contact inspection is usually concerned with measuring or gaging some physical dimension of the part. It is accomplished manually or automatically. Most of the traditional measuring and gaging devices described in Chapter 5 relate to contact inspection. An example of an automated contact measuring system is the coordinate measuring machine (Section 42.6.1). Noncontact inspection methods utilize a sensor located a certain distance from the object to measure or gage the desired feature(s). Typical advantages of noncontact inspection are (1) faster inspection cycles, and (2) avoidance of damage to the part that might result from contact. Noncontact methods can often be accomplished on the production line without any special handling. By contrast, contact inspection usually requires special positioning of the part, necessitating its removal from the production line. Also, noncontact inspection methods are inherently faster because they employ a stationary probe that does not require positioning for every part. By contrast, contact inspection requires positioning of the contact probe against the part, which takes time. Noncontact inspection technologies can be classified as optical or nonoptical. Prominent among the optical methods are lasers (Section 42.6.2) and machine vision (Section 42.6.3). Nonoptical inspection sensors include electrical field techniques, radiation techniques, and ultrasonics (Section 42.6.4). Advanced technologies are substituting for manual measuring and gaging techniques in modern manufacturing plants. They include contact and noncontact sensing methods. We begin our coverage with an important contact inspection technology: coordinate measuring machines. 42.6.1 COORDINATE MEASURING MACHINES A coordinate measuring machine (CMM) consists of a contact probe and a mechanism to position the probe in three dimensions relative to surfaces and features of a workpart. See Figure 42.5. The location coordinates of the probe can be accurately recorded as it contacts the part surface to obtain part geometry data. In a CMM, the probe is fastened to a structure that allows movement of the probe relative to the part, which is fixtured on a worktable connected to the structure. The structure must be rigid to minimize deflections that contribute to measurement errors. The machine in Figure 42.5 has a bridge structure, one of the most common designs. Special features are used in CMM structures to build high accuracy and precision into the measuring machine, including use of low-friction air-bearings and mechanical isolation of the CMM to reduce vibrations. An important aspect in a CMM is the contact probe and FIGURE 42.5 Coordinate measuring machine. (Courtesy of Brown & Sharpe Manufacturing Company, North Kingstown, Rhode Island.) its operation. Modern ‘‘touch-trigger’’ probes have a sensitive electrical contact that emits a signal when the probe is deflected from its neutral position in the slightest amount. On contact, the coordinate positions are recorded by the CMM controller, adjusting for overtravel and probe size. Positioning of the probe relative to the part can be accomplished either manually or under computer control. Methods of operating a CMM can be classified as (1) manual control, (2) manual computer-assisted, (3) motorized computer-assisted, and (4) direct computer control. In manual control, a human operator physically moves the probe along the axes to contact the part and record the measurements. The probe is free-floating for easy movement. Measurements are indicated by digital read-out, and the operator can record the measurement manually or automatically (paper print-out). Any trigonometric calculations must be made by the operator. The manual computer-assisted CMM is capable of computer data processing to perform these calculations. Types of computations include simple conversions from U.S customary units to SI, determining the angle between two planes, and determining hole-center locations. The probe is still free-floating to permit the operator to bring it into contact with part surfaces. Motorized computer-assisted CMMs power drive the probe along the machine axes under operator guidance. A joystick or similar device is used to control the motion. Low-power stepping motors and friction clutches are used to reduce the effects of collisions between probe and part. The direct computer-control CMM operates like a CNC machine tool. It is a computerized inspection machine that operates under program control. The basic capability of a CMM is to determine coordinate values where its probe contacts the surface of a part. Computer control permits the CMM to accomplish more sophisticated measurements and inspections, such as (1) determining center location of a hole or cylinder, (2) definition of a plane, (3) measurement of flatness of a surface or parallelism between two surfaces, and (4) measurement of an angle between two planes. Advantages of using coordinate measuring machines over manual inspection methods include (1) higher productivity—a CMM can perform complex inspection procedures in much less time than traditional manual methods; (2) greater inherent accuracy and precision than conventional methods; and (3) reduced human error through automation of the inspection procedure and associated computations [8]. A CMM is a general-purpose machine that can be used to inspect a variety of part configurations. 42.6.2 MEASUREMENTS WITH LASERS Recall that laser stands for light amplification by stimulated emission of radiation. Applications of lasers include cutting (Section 26.3.3) and welding (Section 30.4). These applications involve the use of solid-state lasers capable of focusing sufficient power to melt or sublimate the work material. Lasers for measurement applications are low-power gas lasers such as helium-neon, which emits light in the visible range. The light beam from a laser is (1) highly monochromatic, which means the light has a single wave length, and (2) highly collimated, which means the light rays are parallel. These properties have motivated a growing list of laser applications in measurement and inspection. We describe two here. Scanning Laser Systems The scanning laser uses a laser beam deflected by a rotating mirror to produce a beam of light that sweeps past an object, as in Figure 42.6. A photodetector on the far side of the object senses the light beam during its sweep except for the short time when it is interrupted by the object. This time period can be measured FIGURE 42.6 Scanning laser system for measuring diameter of cylindrical workpart; time of interruption of light beam is proportional to diameter D. quickly with great accuracy. A microprocessor system measures the time interruption that is related to the size of the object in the path of the laser beam, and converts the time to a linear dimension. Scanning laser beams can be applied in high production on-line inspection and gaging. Signals can be sent to production equipment to make adjustments in the process and/or activate a sortation device on the production line. Applications of scanning laser systems include rolling-mill operations, wire extrusion, machining, and grinding. Laser Triangulation Triangulation is used to determine the distance of an object from two known locations by means of trigonometric relationships of a right triangle. The principle can be applied in dimensional measurements using alaser system, as in Figure 42.7. The laser beam is focused on an object to form a light spot on the surface. A position-sensitive optical detector is used to determine the location of the spot. The angle A of the beam directed at the object and the distance H are fixed and known. Given that the photodetector is located a fixed distance above the worktable, the part depth D in the setup of Figure 42.7 is determined from D ¼ HR ¼ HL cot A ð42:14Þ where L is determined by the position of the light spot on the workpart. 42.6.3 MACHINE VISION Machine vision involves the acquisition, processing, and interpretation of image data by computer for some useful application. Vision systems can be classified as two dimensional or three dimensional. Two-dimensional systems view the scene as a 2-D image, which is quite adequate for applications involving a planar object. Examples include dimensional measuring and gaging, verifying the presence of components, and checking for features on a flat (or almost flat) surface. Three-dimensional vision systems are required for applications requiring a3-D analysis of the scene, wherecontours or shapes areinvolved. Themajorityof current applications are 2-D, and our discussion will focus (excuse the pun) on this technology. Operation of Machine Vision Systems Operation of a machine vision system consists of three steps, depicted in Figure 42.8: (1) image acquisition and digitization, (2) image processing and analysis, and (3) interpretation. Image acquisition and digitizing is accomplished by a video camera connected to a digitizing system to store the image data for subsequent processing. With the camera focused on the subject, an image is obtained by dividingtheviewingareaintoamatrix ofdiscrete picture elements (called pixels), in which each element assumes a value proportional to the FIGURE 42.9 Image acquisition and digitizing: (a) the scene consists of a dark-colored part against alightbackground;(b)a12 12 matrix of pixels imposed on the scene. light intensity of that portion of the scene. The intensity value for each pixel is converted to its equivalent digital value by analog-to-digital conversion. Image acquisition and digitizing is depicted in Figure 42.9 for a binary vision system, in which the light intensity is reduced to either of two values (black or white ¼ 0or 1), as in Table 42.4. The pixel matrix in our illustration is only 12 12; a real vision system would have many more pixels for better resolution. Each set of pixel values is a frame,which consists of the setofdigitized pixel values. The frame is stored in computer memory. The process of reading all the pixel values in a frame is performed 30 times per second in United States, 25 cycle/s in European systems. The resolution of a vision system is its ability to sense fine details and features in the image. This depends on the number of pixelsused. Common pixel arraysinclude 640 (horizontal) 480 (vertical), 1024 768, or 1040 1392 picture elements. The more pixels in the vision system, the higher its resolution. However, system cost increases as pixel count increases. Also, time required to read the pictureelements and process the data increases with number of pixels. In addition to binary vision systems, more sophisticated vision systems distinguish various gray levels in the image thatpermitthem to determine surface characteristics such as texture. Called gray-scale vision,these systems typically use 4,6,or 8 bits of memory. Other vision systems can recognize colors. TABLE 42.4  Pixe l values  in a bin ary visi on syst em for t he imag e in Fig ure  42.8.  1 1 1 1 1 1 1 1 1 1 1 1  1 1 1 1 1 1 1 1 1 1 1 1  1 1 1 1 1 1 1 1 1 1 1 1  1 1 1 1 1 1 1 0 0 1 1 1  1 1 1 1 1 1 0 0 1 0 1 1  1 1 1 1 1 0 0 0 0 0 1 1  1 1 1 1 0 0 0 0 0 0 1 1  1 1 1 0 1 1 0 0 0 0 1 1  1 1 1 0 1 1 0 0 0 0 1 1  1 1 1 0 0 0 0 0 0 0 1 1  1 1 1 1 1 1 1 1 1 1 1 1  1 1 1 1 1 1 1 1 1 1 1 1  The second function in machine vision is image processing and analysis.The data for each frame must be analyzed within the time required to complete one scan (1/30 s or 1/25 s). Several techniques have been developed to analyze image data, including edge detection and feature extraction. Edge detection involves determining the locations of boundaries between an object and its surroundings. This is accomplished by identifying contrast in light intensity between adjacent pixels at the borders of the object. Feature extraction is concerned with determining feature values of an image. Many machine vision systems identify an object in the image by means of its features. Features of an object include area, length, width, or diameter of the object, perimeter, center of gravity, and aspect ratio. Feature extraction algorithms are designed to determine thesefeatures based on the object’s areaand boundaries. Area of an object can be determined by counting the number of pixels that make up the object. Length can be found by measuring the distance (in pixels) between two opposite edges of the part. Interpretationofthe imageis thethird function.Itisaccomplishedbyextractedfeatures. Interpretation is usually concerned with recognizing the object—identifying the object in the image by comparing it to predefined models or standard values. One common interpretation techniqueistemplatematching,whichreferstomethodsthatcompareoneormorefeaturesof an image with corresponding features of a model (template) stored in computer memory. Machine Vision Applications The interpretation function in machine vision is generally related to applications, which divide into four categories: (1) inspection, (2) part identification, (3) visual guidance and control, and (4) safety monitoring. Inspection is the most important category, accounting for about 90% of all industrial applications. The applications are in mass production, where the time to program and install the system can be divided by many thousands of units. Typical inspection tasks include: (1) dimensional measurement or gaging, which involves measuring or gaging certain dimensions of parts or products moving along a conveyor; (2) verification functions, which include verifying presence of components in an assembled product, presence of a hole in a workpart, and similar tasks; and (3) identification of flaws and defects, such as identifying flaws in a printed label in the form of mislocation, poorly printed text, numbering, or graphics on the label. Part identification applications include counting different parts flowing past on a conveyor, part sorting, and character recognition. Visual guidance and control involves a vision system interfaced with a robot or similar machine to control the movement of the machine. Examples include seam tracking in continuous arc welding, part positioning, part reorientation, and picking parts from a bin. In safety monitoring applications, the vision system monitors the production operation to detect irregularities that might indicate a hazardous condition to equipment or humans. 42.6.4 OTHER NONCONTACT INSPECTION TECHNIQUES In addition to optical inspection methods, there are various nonoptical techniques used in inspection. These include sensor techniques based on electrical fields, radiation, and ultrasonics. Under certain conditions, electrical fields created by an electrical probe can be used for inspection. The fields include reluctance, capacitance, and inductance; they are affected by an object in the vicinity of the probe. In a typical application, the workpart is positioned in a fixed relationship to the probe. By measuring the effect of the object on the electrical field, an indirect measurement of certain part characteristics can be made, such as dimensional features, thickness of sheet material, and flaws (cracks and voids below the surface) in the material. Radiation techniques use X-ray radiation to inspect metals and weldments. The amount of radiation absorbed by the metal object indicates thickness and presence of flaws in the part or welded section. For example, X-ray inspection is used to measure thickness of sheet metal in rolling (Section 19.1). Data from the inspection is used to adjust the gap between rolls in the rolling mill. Ultrasonic techniques use high-frequency sound (>20,000 Hz) to perform various inspection tasks. One of the techniques analyses the ultrasonic waves emitted by a probe and reflected off the object. During the setup for the inspection procedure, an ideal test part is positioned in front of the probe to obtain a reflected sound pattern. This sound pattern is used as the standard against which production parts are subsequently compared. If the reflected pattern from a given part matches the standard, the part is accepted. If a match is not obtained, the part is rejected. [1] DeFeo, J. A., Gryna, F. M., and Chua, R. C. H. Juran’s Quality Planning and Analysis for Enterprise Quality, 5th ed., McGraw-Hill, New York, 2006. [2] Evans, J. R., and Lindsay, W. M. The Management and Control of Quality, 6th ed. Thomson/South-Western College Publishing Company, Mason, Ohio, 2005. [3] Groover, M. P. Automation, Production Systems, and Computer Integrated Manufacturing, 3rd ed. Prentice Hall, Upper Saddle River, New Jersey, 2008. [4] Juran, J. M., and Gryna, F. M. Quality Planning and Analysis, 3rd ed. McGraw-Hill, New York, 1993. [5] Lochner, R. H., and Matar, J. E. Designing for Quality. ASQC Quality Press, Milwaukee, Wisconsin, 1990. [6] Montgomery, D. C. Introduction to Statistical Quality Control, 6th ed. John Wiley & Sons, Inc., Hoboken, New Jersey, 2008. [7] Pyzdek, T., and Keller, P. Quality Engineering Handbook. 2nd ed. CRC Taylor & Francis, Boca Raton, Florida, 2003. [8] Schaffer, G. H.‘‘Taking the Measure of CMMs.’’ Special Report 749, American Machinist, October 1982, pp. 145–160. [9] Schaffer, G. H.‘‘Machine Vision: A Sense for CIM.’’ Special Report 767, American Machinist, June 1984, pp. 101–120. [10] Taguchi, G., Elsayed, E. A., and Hsiang, T. C. Quality Engineering in Production Systems. McGraw-Hill, New York, 1989. [11] Wick, C., and Veilleux, R. F. Tool and Manufacturing Engineers Handbook, 4th ed. Vol. IV, Quality Control and Assembly. Society of Manufacturing Engineers, Dearborn, Michigan, 1987. 42.1.  What  are  the  two  principal aspects of product  42.3.  Define process capability.  quality?  42.4.  What are the natural tolerance limits?  42.2.  How is a process operating in statistical control  42.5.  What is the difference between control charts for  distinguished from one that is not?  variables and control charts for attributes?  